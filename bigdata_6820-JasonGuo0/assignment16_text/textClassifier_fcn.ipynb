{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "Thus far we have focused our attention on regression and classification problems involving data characterized by numbers: pixel intensities in images, features described by numerical values.  There have certainly been some cases where some of the feature data was text - for example the housing price data we started with in the first week of class.   What we did with that data was immediately convert that data to a numerical represention.   That was relatively easy, since the text data fell into a small number of discrete categories, which we could easily label with numbers.\n",
    "\n",
    "However, there are a large class of problems where the **only** information we have is textual.   What do we do in this case?\n",
    "\n",
    "We will start our investigation using a well-known dataset: the \"20 newsgroups dataset\".  The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups.\n",
    "\n",
    "I have already read in this data, and put the results into a pickle file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 Newsgroups Labels:\n",
      "    talk.politics.guns\n",
      "    comp.sys.mac.hardware\n",
      "    sci.space\n",
      "    rec.sport.hockey\n",
      "    alt.atheism\n",
      "    sci.med\n",
      "    talk.politics.mideast\n",
      "    comp.sys.ibm.pc.hardware\n",
      "    talk.religion.misc\n",
      "    comp.graphics\n",
      "    talk.politics.misc\n",
      "    sci.electronics\n",
      "    rec.motorcycles\n",
      "    rec.autos\n",
      "    soc.religion.christian\n",
      "    comp.os.ms-windows.misc\n",
      "    sci.crypt\n",
      "    rec.sport.baseball\n",
      "    misc.forsale\n",
      "    comp.windows.x\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#\n",
    "# Load our data\n",
    "fnames,labelnames,texts = pickle.load(open('/fs/scratch/PAS1495/physics6820/text/20_newsgroup/preprocessed/20newsgroups_fnames_labelnames_texts.pkl', 'rb') )\n",
    "#\n",
    "# What are the unique labels?  Convert the label list ot a set!\n",
    "uniqueLabels = set(labelnames)\n",
    "\n",
    "print(\"20 Newsgroups Labels:\")\n",
    "for label in uniqueLabels:\n",
    "    print(\"   \", label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle\n",
    "Shuffle the text and labels coherently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Label comp.os.ms-windows.misc ; encoding [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " Label talk.politics.mideast ; encoding [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " Label soc.religion.christian ; encoding [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " Label comp.sys.ibm.pc.hardware ; encoding [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " Label comp.graphics ; encoding [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " Label comp.windows.x ; encoding [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " Label soc.religion.christian ; encoding [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " Label misc.forsale ; encoding [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " Label sci.crypt ; encoding [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " Label rec.motorcycles ; encoding [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      "\n",
      "This gives us a lookup table which goes from index (position in 1-hot encoding) to label:\n",
      "indexToLabel:  {2: 'comp.os.ms-windows.misc', 17: 'talk.politics.mideast', 15: 'soc.religion.christian', 3: 'comp.sys.ibm.pc.hardware', 1: 'comp.graphics', 5: 'comp.windows.x', 6: 'misc.forsale', 11: 'sci.crypt', 8: 'rec.motorcycles', 7: 'rec.autos', 16: 'talk.politics.guns', 0: 'alt.atheism', 13: 'sci.med', 14: 'sci.space', 12: 'sci.electronics', 4: 'comp.sys.mac.hardware', 10: 'rec.sport.hockey', 9: 'rec.sport.baseball', 19: 'talk.religion.misc', 18: 'talk.politics.misc'}\n",
      "\n",
      "This gives us a lookup table which goes from label back to index:\n",
      "labelToIndex:  {'comp.os.ms-windows.misc': 2, 'talk.politics.mideast': 17, 'soc.religion.christian': 15, 'comp.sys.ibm.pc.hardware': 3, 'comp.graphics': 1, 'comp.windows.x': 5, 'misc.forsale': 6, 'sci.crypt': 11, 'rec.motorcycles': 8, 'rec.autos': 7, 'talk.politics.guns': 16, 'alt.atheism': 0, 'sci.med': 13, 'sci.space': 14, 'sci.electronics': 12, 'comp.sys.mac.hardware': 4, 'rec.sport.hockey': 10, 'rec.sport.baseball': 9, 'talk.religion.misc': 19, 'talk.politics.misc': 18}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import shuffle\n",
    "from keras.utils import to_categorical#\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Do an ordered shuffle of correlated arrays\n",
    "def shuffle_list(*ls):\n",
    "  l =list(zip(*ls))\n",
    "  shuffle(l)\n",
    "  return zip(*l)\n",
    "\n",
    "# Shuffle the labels and text coherently\n",
    "slabels,stexts = shuffle_list(labelnames,texts)\n",
    "\n",
    "#\n",
    "# Convert our labels names to one-hot encoded versions\n",
    "lb = LabelBinarizer()\n",
    "slabels_onehot = lb.fit_transform(slabels)\n",
    "indexToLabel = {}\n",
    "labelToIndex = {}\n",
    "for i in range(len(slabels)):\n",
    "    if i<10:\n",
    "        print(\" Label\",slabels[i],\"; encoding\",slabels_onehot[i])\n",
    "    decVal = lb.inverse_transform(slabels_onehot[[i]])[0]\n",
    "    index = np.argmax(slabels_onehot[i])\n",
    "    indexToLabel[index] = decVal\n",
    "    labelToIndex[decVal] = index\n",
    "print()\n",
    "print(\"This gives us a lookup table which goes from index (position in 1-hot encoding) to label:\")\n",
    "print(\"indexToLabel: \",indexToLabel)\n",
    "print()\n",
    "print(\"This gives us a lookup table which goes from label back to index:\")\n",
    "print(\"labelToIndex: \",labelToIndex)\n",
    "#\n",
    "# Make a train/test split\n",
    "VALIDATION_SPLIT = 0.2\n",
    "num_validation_samples = int(VALIDATION_SPLIT * len(texts))\n",
    "\n",
    "train_posts = stexts[:-num_validation_samples]\n",
    "y_train = slabels_onehot[:-num_validation_samples]\n",
    "test_posts = stexts[-num_validation_samples:]\n",
    "y_test = slabels_onehot[-num_validation_samples:]\n",
    "y_test_labels = slabels[-num_validation_samples:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Tokenize\n",
    "Fo processing text, Keras provides a Tokenizer function.   The idea is to turn the words in our text into:\n",
    "* numbers: each word is converted to a specific number\n",
    "* sequences: each document - which is a list of words in a specific order - is converted to a sequence of numbers in a corresponding specific order.\n",
    "* vectors: each document can be veiwed as a **sparse** vector, with a 1 in the numeric ordered spot if the text contains that word, a zero otherwise\n",
    "\n",
    "The Keras Tokenize class also lowercases and remove punctuations.   \n",
    "\n",
    "Let's see how this works with a list of very short documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indices and document links: \n",
      "  word: physics ; Index: 1 ; documents: 4\n",
      "  word: this ; Index: 2 ; documents: 2\n",
      "  word: is ; Index: 3 ; documents: 2\n",
      "  word: about ; Index: 4 ; documents: 2\n",
      "  word: just ; Index: 5 ; documents: 2\n",
      "  word: and ; Index: 6 ; documents: 2\n",
      "  word: the ; Index: 7 ; documents: 1\n",
      "  word: in ; Index: 8 ; documents: 1\n",
      "  word: a ; Index: 9 ; documents: 1\n",
      "  word: sentence ; Index: 10 ; documents: 1\n",
      "  word: condensed ; Index: 11 ; documents: 1\n",
      "  word: matter ; Index: 12 ; documents: 1\n",
      "  word: i ; Index: 13 ; documents: 1\n",
      "  word: read ; Index: 14 ; documents: 1\n",
      "  word: some ; Index: 15 ; documents: 1\n",
      "  word: papers ; Index: 16 ; documents: 1\n",
      "  word: high ; Index: 17 ; documents: 1\n",
      "  word: energy ; Index: 18 ; documents: 1\n",
      "  word: theoretical ; Index: 19 ; documents: 1\n",
      "  word: more ; Index: 20 ; documents: 1\n",
      "  word: than ; Index: 21 ; documents: 1\n",
      "  word: pushing ; Index: 22 ; documents: 1\n",
      "  word: pencils ; Index: 23 ; documents: 1\n",
      "  word: paper ; Index: 24 ; documents: 1\n",
      "  word: around ; Index: 25 ; documents: 1\n",
      "  word: nodel ; Index: 26 ; documents: 1\n",
      "  word: prize ; Index: 27 ; documents: 1\n",
      "  word: year ; Index: 28 ; documents: 1\n",
      "  word: 2018 ; Index: 29 ; documents: 1\n",
      "  word: went ; Index: 30 ; documents: 1\n",
      "  word: to ; Index: 31 ; documents: 1\n",
      "  word: askin ; Index: 32 ; documents: 1\n",
      "  word: mourou ; Index: 33 ; documents: 1\n",
      "  word: strickland ; Index: 34 ; documents: 1\n",
      "  word: for ; Index: 35 ; documents: 1\n",
      "  word: their ; Index: 36 ; documents: 1\n",
      "  word: groundbreaking ; Index: 37 ; documents: 1\n",
      "  word: inventions ; Index: 38 ; documents: 1\n",
      "  word: field ; Index: 39 ; documents: 1\n",
      "  word: of ; Index: 40 ; documents: 1\n",
      "  word: laser ; Index: 41 ; documents: 1\n",
      "\n",
      "Orginal Text:  This is a sentence about condensed matter physics.\n",
      "As a vector:   [0. 1. 1. 1. 1.]\n",
      "As a sequence  [2, 3, 4, 1]\n",
      "\n",
      "Orginal Text:  I just read some papers about high energy physics.\n",
      "As a vector:   [0. 1. 0. 0. 1.]\n",
      "As a sequence  [4, 1]\n",
      "\n",
      "Orginal Text:  Theoretical physics is more than just pushing pencils and paper around.\n",
      "As a vector:   [0. 1. 0. 1. 0.]\n",
      "As a sequence  [1, 3]\n",
      "\n",
      "Orginal Text:  The Nodel Prize in Physics this year (2018) went to Askin, Mourou, and Strickland for their groundbreaking inventions in the field of laser physics\n",
      "As a vector:   [0. 1. 1. 0. 0.]\n",
      "As a sequence  [1, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "docs = [\"This is a sentence about condensed matter physics.\",\n",
    "           \"I just read some papers about high energy physics.\",\n",
    "           \"Theoretical physics is more than just pushing pencils and paper around.\",\n",
    "           \"The Nodel Prize in Physics this year (2018) went to Askin, Mourou, and Strickland for their groundbreaking inventions in the field of laser physics\"]\n",
    "\n",
    "vocab_size = 5\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "#\n",
    "# Get summary info from the tokenizer\n",
    "print(\"Word indices and document links: \")\n",
    "for word in tokenizer.word_index:\n",
    "    print(\"  word:\",word,\"; Index:\",tokenizer.word_index[word],\"; documents:\",tokenizer.word_docs[word])\n",
    "\n",
    "# integer encode documents\n",
    "#  mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n",
    "encoded_docs_matrix = tokenizer.texts_to_matrix(docs, mode='binary')\n",
    "encoded_docs_sequence = tokenizer.texts_to_sequences(docs)\n",
    "for docnum in range(len(docs)):\n",
    "    print()\n",
    "    print(\"Orginal Text: \",docs[docnum])\n",
    "    print(\"As a vector:  \",encoded_docs_matrix[docnum])\n",
    "    print(\"As a sequence \", encoded_docs_sequence[docnum])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True vocabulary size: 150693\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# 20 news groups\n",
    "num_labels = len(uniqueLabels)\n",
    "vocab_size = 10000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    "\n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    "\n",
    "print(\"True vocabulary size:\",len(tokenizer.word_index))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Keras Model\n",
    "This is just a simple fully connected network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1000100   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 1,006,170\n",
      "Trainable params: 1,006,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 15063 samples, validate on 3765 samples\n",
      "Epoch 1/30\n",
      "15063/15063 [==============================] - 4s 268us/step - loss: 1.6246 - acc: 0.5557 - val_loss: 0.6744 - val_acc: 0.8417\n",
      "Epoch 2/30\n",
      "15063/15063 [==============================] - 4s 257us/step - loss: 0.5304 - acc: 0.8634 - val_loss: 0.5412 - val_acc: 0.8627\n",
      "Epoch 3/30\n",
      "15063/15063 [==============================] - 4s 254us/step - loss: 0.2885 - acc: 0.9284 - val_loss: 0.5446 - val_acc: 0.8701\n",
      "Epoch 4/30\n",
      "15063/15063 [==============================] - 4s 250us/step - loss: 0.1870 - acc: 0.9554 - val_loss: 0.5399 - val_acc: 0.8717\n",
      "Epoch 5/30\n",
      "15063/15063 [==============================] - 4s 256us/step - loss: 0.1499 - acc: 0.9645 - val_loss: 0.5679 - val_acc: 0.8683\n",
      "Epoch 6/30\n",
      "15063/15063 [==============================] - 4s 258us/step - loss: 0.1160 - acc: 0.9748 - val_loss: 0.5848 - val_acc: 0.8680\n",
      "Epoch 7/30\n",
      "15063/15063 [==============================] - 4s 258us/step - loss: 0.1005 - acc: 0.9790 - val_loss: 0.5947 - val_acc: 0.8725\n",
      "Epoch 8/30\n",
      "15063/15063 [==============================] - 4s 251us/step - loss: 0.0754 - acc: 0.9837 - val_loss: 0.5942 - val_acc: 0.8728\n",
      "Best validation accuracy is: 0.8717131430251032\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding, Flatten, Dropout\n",
    "from keras.models import Model\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "##### import keras\n",
    "model = Sequential()\n",
    "model.add(Dense(100, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(50, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "callbacks_list = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath='best_model_20newsgroups.mlp.h5',\n",
    "        monitor='val_loss', save_best_only=True),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=4)\n",
    "]\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks_list,\n",
    "                    validation_data=(x_test, y_test))\n",
    "patience = 4\n",
    "best_val_acc =  history.history['val_acc'][-(patience+1)]\n",
    "print(\"Best validation accuracy is:\",best_val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "We look at a few things:\n",
    "1.  The standard confusion matrix.\n",
    "2.  The number of classes which were predicted incorrectly (where the highest predicted probability class was wrong) *AND* the subset of these in which the 2nd highest predicted probability class was correct.   If this was random we would expect that about 10% (since there are 10 remaining classes) of the number incorrectly predicted would have the second highest probability be correct.   Instead this number is more like 60%.\n",
    "3.  A modified confusion matrix where we count the correct classifications to be those in which one of the two highest predicted classes was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUE:  sci.space ;\t PREDICTED:  sci.space\n",
      "TRUE:  sci.electronics ;\t PREDICTED:  sci.electronics\n",
      "TRUE:  misc.forsale ;\t PREDICTED:  sci.electronics\n",
      "TRUE:  comp.sys.ibm.pc.hardware ;\t PREDICTED:  comp.sys.ibm.pc.hardware\n",
      "TRUE:  comp.sys.mac.hardware ;\t PREDICTED:  comp.sys.mac.hardware\n",
      "TRUE:  sci.med ;\t PREDICTED:  sci.med\n",
      "TRUE:  comp.windows.x ;\t PREDICTED:  comp.windows.x\n",
      "TRUE:  comp.sys.ibm.pc.hardware ;\t PREDICTED:  comp.sys.ibm.pc.hardware\n",
      "TRUE:  comp.sys.ibm.pc.hardware ;\t PREDICTED:  comp.os.ms-windows.misc\n",
      "TRUE:  sci.space ;\t PREDICTED:  sci.space\n",
      "\n",
      "Standard confusion matrix and accuracy when using the highest predicted prob to get predicted class.\n",
      "talk.politics.guns \t0.92 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.01 \t0.0 \t0.04 \t0.0 \t0.01 \t0.01 \t0.0 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t\n",
      "comp.sys.mac.hardware \t0.01 \t0.8 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.07 \t0.0 \t0.02 \t0.01 \t0.03 \t0.01 \t0.01 \t0.0 \t0.01 \t0.0 \t0.0 \t0.03 \t0.01 \t\n",
      "sci.space \t0.0 \t0.0 \t0.95 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.01 \t0.01 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t\n",
      "rec.sport.hockey \t0.0 \t0.0 \t0.0 \t0.97 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t\n",
      "alt.atheism \t0.0 \t0.0 \t0.01 \t0.0 \t0.81 \t0.01 \t0.03 \t0.0 \t0.09 \t0.0 \t0.0 \t0.0 \t0.0 \t0.01 \t0.04 \t0.01 \t0.0 \t0.0 \t0.01 \t0.0 \t\n",
      "sci.med \t0.01 \t0.01 \t0.02 \t0.0 \t0.0 \t0.9 \t0.0 \t0.01 \t0.0 \t0.0 \t0.01 \t0.02 \t0.0 \t0.01 \t0.01 \t0.01 \t0.0 \t0.01 \t0.02 \t0.01 \t\n",
      "talk.politics.mideast \t0.0 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.94 \t0.0 \t0.01 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.01 \t0.01 \t0.01 \t0.0 \t\n",
      "comp.sys.ibm.pc.hardware \t0.0 \t0.05 \t0.01 \t0.0 \t0.0 \t0.0 \t0.0 \t0.78 \t0.01 \t0.02 \t0.0 \t0.04 \t0.0 \t0.01 \t0.0 \t0.04 \t0.0 \t0.0 \t0.06 \t0.01 \t\n",
      "talk.religion.misc \t0.05 \t0.0 \t0.01 \t0.0 \t0.04 \t0.01 \t0.01 \t0.0 \t0.75 \t0.0 \t0.02 \t0.02 \t0.0 \t0.0 \t0.08 \t0.0 \t0.0 \t0.0 \t0.0 \t0.02 \t\n",
      "comp.graphics \t0.0 \t0.02 \t0.01 \t0.0 \t0.0 \t0.01 \t0.01 \t0.04 \t0.01 \t0.8 \t0.0 \t0.02 \t0.0 \t0.0 \t0.0 \t0.03 \t0.01 \t0.0 \t0.03 \t0.03 \t\n",
      "talk.politics.misc \t0.07 \t0.0 \t0.01 \t0.0 \t0.0 \t0.02 \t0.01 \t0.0 \t0.02 \t0.01 \t0.84 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.01 \t0.0 \t\n",
      "sci.electronics \t0.01 \t0.01 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.05 \t0.0 \t0.0 \t0.0 \t0.8 \t0.01 \t0.01 \t0.0 \t0.01 \t0.01 \t0.0 \t0.05 \t0.01 \t\n",
      "rec.motorcycles \t0.01 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.94 \t0.02 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t\n",
      "rec.autos \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.01 \t0.0 \t0.9 \t0.0 \t0.0 \t0.0 \t0.0 \t0.03 \t0.01 \t\n",
      "soc.religion.christian \t0.0 \t0.0 \t0.01 \t0.0 \t0.01 \t0.01 \t0.01 \t0.0 \t0.04 \t0.01 \t0.01 \t0.0 \t0.01 \t0.01 \t0.91 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t\n",
      "comp.os.ms-windows.misc \t0.0 \t0.01 \t0.01 \t0.01 \t0.0 \t0.0 \t0.0 \t0.04 \t0.0 \t0.05 \t0.01 \t0.02 \t0.0 \t0.0 \t0.01 \t0.78 \t0.0 \t0.0 \t0.03 \t0.04 \t\n",
      "sci.crypt \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.01 \t0.01 \t0.01 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.92 \t0.0 \t0.0 \t0.01 \t\n",
      "rec.sport.baseball \t0.0 \t0.0 \t0.01 \t0.02 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.0 \t0.02 \t0.01 \t0.0 \t0.0 \t0.95 \t0.0 \t0.0 \t\n",
      "misc.forsale \t0.0 \t0.02 \t0.0 \t0.0 \t0.0 \t0.01 \t0.01 \t0.03 \t0.0 \t0.02 \t0.0 \t0.02 \t0.01 \t0.03 \t0.0 \t0.0 \t0.01 \t0.0 \t0.86 \t0.0 \t\n",
      "comp.windows.x \t0.0 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.01 \t0.02 \t0.01 \t0.04 \t0.0 \t0.01 \t0.0 \t0.0 \t0.0 \t0.02 \t0.01 \t0.0 \t0.01 \t0.87 \t\n",
      "\n",
      "Average accuracy: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/python/3.6-conda5.2/lib/python3.6/site-packages/matplotlib/axes/_axes.py:6571: UserWarning: The 'normed' kwarg is deprecated, and has been replaced by the 'density' kwarg.\n",
      "  warnings.warn(\"The 'normed' kwarg is deprecated, and has been \"\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#\n",
    "# Get our predictions for each sample in the test sample\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "#\n",
    "# Containers to store some resuls\n",
    "trueCount = defaultdict(int)\n",
    "confusionMatrix = defaultdict(partial(defaultdict, int))\n",
    "probsWhenRight = []\n",
    "\n",
    "#\n",
    "# Loop over data and collect results\n",
    "for i in range(len(y_test_labels)):\n",
    "    t_test_index = labelToIndex[y_test_labels[i]]\n",
    "    trueCount[y_test_labels[i]] += 1\n",
    "    predLabelIndex = np.argmax(y_test_pred[i])\n",
    "    predLabel = indexToLabel[predLabelIndex]\n",
    "    predictedProbability = y_test_pred[i][predLabelIndex]\n",
    "    if i<10:\n",
    "        print(\"TRUE: \",y_test_labels[i],\";\\t PREDICTED: \",predLabel)\n",
    "    confusionMatrix[y_test_labels[i]][predLabel] += 1\n",
    "#\n",
    "# Store the probability when correct\n",
    "    if y_test_labels[i] == predLabel:\n",
    "        probsWhenRight.append(predictedProbability)\n",
    "    \n",
    "print()\n",
    "print(\"Standard confusion matrix and accuracy when using the highest predicted prob to get predicted class.\")\n",
    "averageAcc = 0.0\n",
    "num = 0.0\n",
    "for i in uniqueLabels:\n",
    "    print(i,'\\t',end='')\n",
    "    for j in uniqueLabels:\n",
    "        print(round(float(confusionMatrix[i][j])/float(trueCount[i]),2),'\\t',end='')\n",
    "        if i==j:\n",
    "            averageAcc += float(confusionMatrix[i][j])/float(trueCount[i])\n",
    "            num += 1.0\n",
    "    print()\n",
    "print()\n",
    "print(\"Average accuracy:\",round(averageAcc/num,4))\n",
    "\n",
    "plt.hist(probsWhenRight, 50, alpha=0.5, normed=True, label='probsWhenRight')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA Credit (1 point each)\n",
    "You can do these things to get extra credit!\n",
    "1.  Add to the same plot above: The probabilities when the predicted class is wrong\n",
    "2.  Count the number that are incorrectly predicted, and for this group, count up how often the correct class is the 2nd highest probability.   This [post](https://stackoverflow.com/questions/6910641/how-do-i-get-indices-of-n-maximum-values-in-a-numpy-array) may be helpful (it is what I used)!  Is the second highest correct at a random rate?  Or is it much higher?\n",
    "3.  Make a modified confusion matrix where we count the correct classifications to be those in which one of the two highest predicted classes was correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
