{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "For this assigment, we are going to do something like k-fold validation to determine the true expected performance of our model\n",
    "\n",
    "The reason this is a little tricky is due to the fact that we would like to our test and train samples to always have different users.   As we noted in the **prep** workbook, every 4 users is about 10% of the sample, so lets use this to divide our data and determine our average performance of our model.\n",
    "\n",
    "**TASK**:\n",
    "To make this concrete, I want you to divide the full sample up into 9 folds, like this:\n",
    "* fold1: test sample is users >=0 and <4; train sample is users>=4\n",
    "* fold2: test sample is users >=4 and <8; train sample is users<4 or users>=8\n",
    "...etc...\n",
    "\n",
    "Determine the performance of the CNN version of the model (use the model made using the Keras Functional API) for each fold, then average the results.\n",
    "\n",
    "**EXTRA**: Incorporate a multi-head model (with at least 3 heads) each using a different kernel size.  Do the averaging as above.   Can you come up with hyperparamters that beat the performance of the earlier 2-headed model (though that was measured with just a single fold).\n",
    "\n",
    "The assigment below has some starter code in to help you begin.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to read data in and normalize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user-id activity       timestamp    x-axis    y-axis    z-axis  \\\n",
      "0       33  Jogging  49105962326000 -0.034732  0.634027  0.025198   \n",
      "1       33  Jogging  49106062271000  0.250614  0.563201  0.047671   \n",
      "2       33  Jogging  49106112167000  0.245166  0.544133 -0.004086   \n",
      "3       33  Jogging  49106222305000 -0.030646  0.924822  0.151186   \n",
      "4       33  Jogging  49106332290000 -0.059249  0.605424  0.360258   \n",
      "\n",
      "   ActivityEncoded  \n",
      "0                1  \n",
      "1                1  \n",
      "2                1  \n",
      "3                1  \n",
      "4                1  \n",
      "max values  0.9974999999999999 1.002 0.9804999999999999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#\n",
    "# Use this to convert text to floating point\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float(x)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "column_names = ['user-id',\n",
    "                    'activity',\n",
    "                    'timestamp',\n",
    "                    'x-axis',\n",
    "                    'y-axis',\n",
    "                    'z-axis']\n",
    "df = pd.read_csv('/fs/scratch/PAS1495/physics6820/WISDM/WISDM_ar_v1.1/WISDM_ar_v1.1_raw.txt',\n",
    "                     header=None,\n",
    "                     names=column_names)\n",
    "\n",
    "# Last column has a \";\" character which must be removed ...\n",
    "df['z-axis'].replace(regex=True,\n",
    "      inplace=True,\n",
    "      to_replace=r';',\n",
    "      value=r'')\n",
    "    # ... and then this column must be transformed to float explicitly\n",
    "df['z-axis'] = df['z-axis'].apply(convert_to_float)\n",
    "    # This is very important otherwise the model will not fit and loss\n",
    "    # will show up as NAN\n",
    "#\n",
    "# Get rid if rows wth missing data\n",
    "df.dropna(axis=0, how='any', inplace=True)\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# Define column name of the label vector\n",
    "LABEL = 'ActivityEncoded'\n",
    "# Transform the labels from String to Integer via LabelEncoder\n",
    "le = preprocessing.LabelEncoder()\n",
    "# Add a new column to the existing DataFrame with the encoded values\n",
    "df[LABEL] = le.fit_transform(df['activity'].values.ravel())\n",
    "\n",
    "#\n",
    "# Normalize the data: to make things simple, just normalize all of the data (pre train/test) by 20\n",
    "max_all = 20.0\n",
    "df['x-axis'] = df['x-axis'] / 20.0\n",
    "df['y-axis'] = df['y-axis'] / 20.0\n",
    "df['z-axis'] = df['z-axis'] / 20.0\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "max_x = df['x-axis'].max()\n",
    "max_y = df['y-axis'].max()\n",
    "max_z = df['z-axis'].max()\n",
    "\n",
    "print(\"max values \", max_x,max_y,max_z)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to create test/train samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Same labels will be reused throughout the program\n",
    "LABELS = ['Downstairs',\n",
    "          'Jogging',\n",
    "          'Sitting',\n",
    "          'Standing',\n",
    "          'Upstairs',\n",
    "          'Walking']\n",
    "# The number of steps within one time segment\n",
    "TIME_PERIODS = 80    # since there are 50 measurements/sec, this is 1.6 seconds of data\n",
    "# The steps to take from one segment to the next; if this value is equal to\n",
    "# TIME_PERIODS, then there is no overlap between the segments\n",
    "STEP_DISTANCE_TRAIN = 40\n",
    "STEP_DISTANCE_TEST = 80\n",
    "\n",
    "def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "\n",
    "    # x, y, z acceleration as features\n",
    "    N_FEATURES = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x-axis'].values[i: i + time_steps]\n",
    "        ys = df['y-axis'].values[i: i + time_steps]\n",
    "        zs = df['z-axis'].values[i: i + time_steps]\n",
    "        # Retrieve the most often used label in this segment\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0] #stats.mode finds the most common values that occurs in the array\n",
    "        #The default axis is 0\n",
    "        \n",
    "        segments.append([xs, ys, zs]) #This segments is a list of lists (nx3). xs, ys, zs are the arrays.\n",
    "        labels.append(label)\n",
    "\n",
    "    # Bring the segments into a better shape\n",
    "    reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, time_steps, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method to initialize weights\n",
    "You must to something like this before fitting your model if you do it in a loop.  Otherwize the weights will not change from loop to loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def shuffle_weights(model, weights=None):\n",
    "    \"\"\"Randomly permute the weights in `model`, or the given `weights`.\n",
    "    This is a fast approximation of re-initializing the weights of a model.\n",
    "    Assumes weights are distributed independently of the dimensions of the weight tensors\n",
    "      (i.e., the weights have the same distribution along each dimension).\n",
    "    :param Model model: Modify the weights of the given model.\n",
    "    :param list(ndarray) weights: The model's weights will be replaced by a random permutation of these weights.\n",
    "      If `None`, permute the model's current weights.\n",
    "    \"\"\"\n",
    "    if weights is None:\n",
    "        weights = model.get_weights()\n",
    "    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n",
    "    # Faster, but less random: only permutes along the first dimension\n",
    "    # weights = [np.random.permutation(w) for w in weights]\n",
    "    model.set_weights(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 80, 3)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 71, 100)           3100      \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 62, 100)           100100    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 20, 100)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 11, 160)           160160    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 2, 160)            256160    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_3 ( (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 520,486\n",
      "Trainable params: 520,486\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input,Conv1D, MaxPooling1D,GlobalAveragePooling1D,Dropout,Dense\n",
    "from keras.models import Model\n",
    "\n",
    "num_classes = 6 \n",
    "# Our first layer gets the input from our samples - this is 80 time steps by 3 channels\n",
    "#model_m.add(Conv1D(100, 10, activation='relu', input_shape=(80,3)))\n",
    "inputs1 = Input(shape=(80,3))\n",
    "conv1 = Conv1D(100, 10, activation='relu')(inputs1)\n",
    "#\n",
    "# Anoth convolutional layer\n",
    "#model_m.add(Conv1D(100, 10, activation='relu'))\n",
    "conv2 = Conv1D(100, 10, activation='relu')(conv1)\n",
    "#\n",
    "# Max pooling \n",
    "#model_m.add(MaxPooling1D(3))\n",
    "pool1 = MaxPooling1D(3)(conv2)\n",
    "#\n",
    "# Two more convolutional layers\n",
    "#model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "#model_m.add(Conv1D(160, 10, activation='relu'))\n",
    "conv3 = Conv1D(160, 10, activation='relu')(pool1)\n",
    "conv4 = Conv1D(160, 10, activation='relu')(conv3)\n",
    "#\n",
    "# Global average pooling use this instead of \"Flatten\" - it helps reduce overfitting\n",
    "#model_m.add(GlobalAveragePooling1D())\n",
    "glob1 = GlobalAveragePooling1D()(conv4)\n",
    "#\n",
    "drop1 = Dropout(0.5)(glob1)\n",
    "outputs = Dense(num_classes, activation='softmax')(drop1)\n",
    "\n",
    "#\n",
    "# Now define the model\n",
    "model_m = Model(inputs=inputs1, outputs=outputs)\n",
    "print(model_m.summary())    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK: \n",
    "\n",
    "Divide the full sample up into 9 folds, like this:\n",
    "\n",
    "*  fold1: test sample is users >=0 and <4; train sample is users>=4\n",
    "*  fold2: test sample is users >=4 and <8; train sample is users<4 or users>=8 \n",
    "*  ...etc...\n",
    "\n",
    "Determine the performance of the CNN version of the model (use the model made using the Keras Functional API) for each fold, then average the results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example pseudo-code\n",
    "We have 36 users.   So lets group them in steps of 4, and use each group of 4 as our test, and the others as our train.\n",
    "\n",
    "Fill in the rest of the code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing the  0 th validation\n",
      "The test users are:\n",
      " [1, 2, 3, 4]\n",
      "The training users are:\n",
      " [0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (998168, 7) Test sample size (100035, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis    z-axis  \\\n",
      "371929        5  Walking  1202312298000 -0.0475  0.2505  0.082403   \n",
      "371930        5  Walking  1202362285000 -0.0805  0.2640  0.028603   \n",
      "371931        5  Walking  1202412304000 -0.1585  0.3750 -0.002043   \n",
      "371932        5  Walking  1202462261000 -0.0230  0.9045 -0.055843   \n",
      "371933        5  Walking  1202512310000  0.3580  0.8715  0.021112   \n",
      "\n",
      "        ActivityEncoded  \n",
      "371929                5  \n",
      "371930                5  \n",
      "371931                5  \n",
      "371932                5  \n",
      "371933                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  1 th validation\n",
      "The test users are:\n",
      " [5, 6, 7, 8]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (967835, 7) Test sample size (130368, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  2 th validation\n",
      "The test users are:\n",
      " [9, 10, 11, 12]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (986098, 7) Test sample size (112105, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  3 th validation\n",
      "The test users are:\n",
      " [13, 14, 15, 16]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (974294, 7) Test sample size (123909, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  4 th validation\n",
      "The test users are:\n",
      " [17, 18, 19, 20]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (941357, 7) Test sample size (156846, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  5 th validation\n",
      "The test users are:\n",
      " [21, 22, 23, 24]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (989048, 7) Test sample size (109155, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  6 th validation\n",
      "The test users are:\n",
      " [25, 26, 27, 28]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 29, 30, 31, 32, 33, 34, 35, 36]\n",
      "Training sample size (995841, 7) Test sample size (102362, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  7 th validation\n",
      "The test users are:\n",
      " [29, 30, 31, 32]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 33, 34, 35, 36]\n",
      "Training sample size (954835, 7) Test sample size (143368, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n",
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n",
      "Doing the  8 th validation\n",
      "The test users are:\n",
      " [33, 34, 35, 36]\n",
      "The training users are:\n",
      " [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "Training sample size (978148, 7) Test sample size (120055, 7)\n",
      "        user-id activity      timestamp  x-axis  y-axis  z-axis  \\\n",
      "941960        1  Walking  4991922345000  0.0345  0.5400 -0.1015   \n",
      "941961        1  Walking  4991972333000  0.3425  0.3720 -0.0250   \n",
      "941962        1  Walking  4992022351000  0.0465  0.2815 -0.0250   \n",
      "941963        1  Walking  4992072339000 -0.1055  0.2505 -0.0345   \n",
      "941964        1  Walking  4992122358000 -0.2295  0.2145 -0.0975   \n",
      "\n",
      "        ActivityEncoded  \n",
      "941960                5  \n",
      "941961                5  \n",
      "941962                5  \n",
      "941963                5  \n",
      "941964                5  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5 ... 3 3 3] [[0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "TIME_PERIODS = 80\n",
    "STEP_DISTANCE_TRAIN = 40\n",
    "STEP_DISTANCE_TEST = 80\n",
    "\n",
    "x_train_list = []\n",
    "train_onehot_list = []\n",
    "x_test_list = []\n",
    "test_onehot_list = []\n",
    "\n",
    "for user_groups in range(9):\n",
    "    user_start = user_groups * 4 + 1\n",
    "    user_end = user_start + 4\n",
    "    \n",
    "    test_id = [i for i in range(user_start, user_end)]\n",
    "    train_id = [i for i in range(0, user_start)]\n",
    "    for i in range(user_end, 37):\n",
    "        train_id.append(i)\n",
    "    \n",
    "    print(\"Doing the \", user_groups, 'th validation')\n",
    "    print(\"The test users are:\\n\", test_id)\n",
    "    print(\"The training users are:\\n\", train_id)\n",
    "    \n",
    "    df_train = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "    for i in train_id:\n",
    "        df_train = pd.concat([df_train, df[df['user-id'] == i]])\n",
    "        \n",
    "    for i in test_id:\n",
    "        df_test = pd.concat([df_test, df[df['user-id'] == i]])\n",
    "        \n",
    "    print(\"Training sample size\", df_train.shape, \"Test sample size\", df_test.shape)\n",
    "    print(df_train.head())\n",
    "    #def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "    x_train, y_train = create_segments_and_labels(df_train, TIME_PERIODS, STEP_DISTANCE_TRAIN, LABEL)\n",
    "    x_test, y_test = create_segments_and_labels(df_train, TIME_PERIODS, STEP_DISTANCE_TEST, LABEL)\n",
    "    \n",
    "    train_onehot = np.zeros((y_train.shape[0], 6))\n",
    "    test_onehot = np.zeros((y_test.shape[0], 6))\n",
    "    for i in range(len(y_train)):\n",
    "        train_onehot[i][y_train[i]] = 1\n",
    "        \n",
    "    for i in range(len(y_test)):\n",
    "        test_onehot[i][y_test[i]] = 1\n",
    "        \n",
    "    print(y_train, train_onehot)\n",
    "    x_train_list.append(x_train)\n",
    "    x_test_list.append(x_test)\n",
    "    train_onehot_list.append(train_onehot)\n",
    "    test_onehot_list.append(test_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24953 samples, validate on 12477 samples\n",
      "Epoch 1/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.7696 - acc: 0.7216 - val_loss: 0.4643 - val_acc: 0.8250\n",
      "Epoch 2/50\n",
      "24953/24953 [==============================] - 58s 2ms/step - loss: 0.4466 - acc: 0.8343 - val_loss: 0.3960 - val_acc: 0.8464\n",
      "Epoch 3/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.3682 - acc: 0.8650 - val_loss: 0.3482 - val_acc: 0.8698\n",
      "Epoch 4/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.3015 - acc: 0.8914 - val_loss: 0.2347 - val_acc: 0.9202\n",
      "Epoch 5/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.2499 - acc: 0.9149 - val_loss: 0.2135 - val_acc: 0.9273\n",
      "Epoch 6/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.2178 - acc: 0.9279 - val_loss: 0.1773 - val_acc: 0.9422\n",
      "Epoch 7/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.1933 - acc: 0.9372 - val_loss: 0.1573 - val_acc: 0.9476\n",
      "Epoch 8/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.1699 - acc: 0.9449 - val_loss: 0.1271 - val_acc: 0.9594\n",
      "Epoch 9/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.1496 - acc: 0.9521 - val_loss: 0.1240 - val_acc: 0.9598\n",
      "Epoch 10/50\n",
      "24953/24953 [==============================] - 54s 2ms/step - loss: 0.1365 - acc: 0.9564 - val_loss: 0.1098 - val_acc: 0.9627\n",
      "Epoch 11/50\n",
      "24953/24953 [==============================] - 50s 2ms/step - loss: 0.1237 - acc: 0.9612 - val_loss: 0.1045 - val_acc: 0.9627\n",
      "Epoch 12/50\n",
      "24953/24953 [==============================] - 55s 2ms/step - loss: 0.1153 - acc: 0.9613 - val_loss: 0.0856 - val_acc: 0.9707\n",
      "Epoch 13/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.1024 - acc: 0.9656 - val_loss: 0.0762 - val_acc: 0.9739\n",
      "Epoch 14/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.0941 - acc: 0.9687 - val_loss: 0.0731 - val_acc: 0.9739\n",
      "Epoch 15/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.0818 - acc: 0.9735 - val_loss: 0.0723 - val_acc: 0.9748\n",
      "Epoch 16/50\n",
      "24953/24953 [==============================] - 53s 2ms/step - loss: 0.0727 - acc: 0.9762 - val_loss: 0.0605 - val_acc: 0.9798\n",
      "Epoch 17/50\n",
      "24953/24953 [==============================] - 55s 2ms/step - loss: 0.0698 - acc: 0.9773 - val_loss: 0.0511 - val_acc: 0.9835\n",
      "Epoch 18/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.0617 - acc: 0.9805 - val_loss: 0.0371 - val_acc: 0.9883\n",
      "Epoch 19/50\n",
      "24953/24953 [==============================] - 50s 2ms/step - loss: 0.0599 - acc: 0.9810 - val_loss: 0.0348 - val_acc: 0.9910\n",
      "Epoch 20/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.0516 - acc: 0.9836 - val_loss: 0.0379 - val_acc: 0.9896\n",
      "Epoch 21/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.0484 - acc: 0.9849 - val_loss: 0.0324 - val_acc: 0.9907\n",
      "Epoch 22/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.0401 - acc: 0.9884 - val_loss: 0.0232 - val_acc: 0.9943\n",
      "Epoch 23/50\n",
      "24953/24953 [==============================] - 53s 2ms/step - loss: 0.0344 - acc: 0.9895 - val_loss: 0.0257 - val_acc: 0.9929\n",
      "Epoch 24/50\n",
      "24953/24953 [==============================] - 53s 2ms/step - loss: 0.0378 - acc: 0.9889 - val_loss: 0.0198 - val_acc: 0.9944\n",
      "Epoch 25/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.0363 - acc: 0.9887 - val_loss: 0.0204 - val_acc: 0.9943\n",
      "Epoch 26/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.0282 - acc: 0.9919 - val_loss: 0.0228 - val_acc: 0.9918\n",
      "Epoch 27/50\n",
      "24953/24953 [==============================] - 58s 2ms/step - loss: 0.0246 - acc: 0.9930 - val_loss: 0.0141 - val_acc: 0.9962\n",
      "Epoch 28/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.0255 - acc: 0.9929 - val_loss: 0.0213 - val_acc: 0.9937\n",
      "Epoch 29/50\n",
      "24953/24953 [==============================] - 51s 2ms/step - loss: 0.0270 - acc: 0.9915 - val_loss: 0.0145 - val_acc: 0.9962\n",
      "Epoch 30/50\n",
      "24953/24953 [==============================] - 52s 2ms/step - loss: 0.0220 - acc: 0.9934 - val_loss: 0.0198 - val_acc: 0.9940\n",
      "Epoch 31/50\n",
      "24953/24953 [==============================] - 54s 2ms/step - loss: 0.0346 - acc: 0.9886 - val_loss: 0.0214 - val_acc: 0.9929\n",
      "Best validation accuracy is: 0.9961529249390931\n",
      "Train on 24194 samples, validate on 12097 samples\n",
      "Epoch 1/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.7689 - acc: 0.7263 - val_loss: 0.4034 - val_acc: 0.8545\n",
      "Epoch 2/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.3782 - acc: 0.8615 - val_loss: 0.2983 - val_acc: 0.8853\n",
      "Epoch 3/50\n",
      "24194/24194 [==============================] - 50s 2ms/step - loss: 0.2573 - acc: 0.9128 - val_loss: 0.1877 - val_acc: 0.9387\n",
      "Epoch 4/50\n",
      "24194/24194 [==============================] - 51s 2ms/step - loss: 0.1941 - acc: 0.9377 - val_loss: 0.1433 - val_acc: 0.9539\n",
      "Epoch 5/50\n",
      "24194/24194 [==============================] - 52s 2ms/step - loss: 0.1638 - acc: 0.9465 - val_loss: 0.1230 - val_acc: 0.9581\n",
      "Epoch 6/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.1361 - acc: 0.9561 - val_loss: 0.1085 - val_acc: 0.9611\n",
      "Epoch 7/50\n",
      "24194/24194 [==============================] - 56s 2ms/step - loss: 0.1169 - acc: 0.9627 - val_loss: 0.0832 - val_acc: 0.9721\n",
      "Epoch 8/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.1033 - acc: 0.9673 - val_loss: 0.0799 - val_acc: 0.9726\n",
      "Epoch 9/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.0868 - acc: 0.9713 - val_loss: 0.0602 - val_acc: 0.9798\n",
      "Epoch 10/50\n",
      "24194/24194 [==============================] - 51s 2ms/step - loss: 0.0793 - acc: 0.9738 - val_loss: 0.0546 - val_acc: 0.9802\n",
      "Epoch 11/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.0672 - acc: 0.9788 - val_loss: 0.0439 - val_acc: 0.9856\n",
      "Epoch 12/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.0554 - acc: 0.9832 - val_loss: 0.0375 - val_acc: 0.9880\n",
      "Epoch 13/50\n",
      "24194/24194 [==============================] - 49s 2ms/step - loss: 0.0540 - acc: 0.9829 - val_loss: 0.0433 - val_acc: 0.9857\n",
      "Epoch 14/50\n",
      "24194/24194 [==============================] - 50s 2ms/step - loss: 0.0414 - acc: 0.9874 - val_loss: 0.0423 - val_acc: 0.9857\n",
      "Epoch 15/50\n",
      "24194/24194 [==============================] - 52s 2ms/step - loss: 0.0383 - acc: 0.9878 - val_loss: 0.0227 - val_acc: 0.9929\n",
      "Epoch 16/50\n",
      "24194/24194 [==============================] - 48s 2ms/step - loss: 0.0316 - acc: 0.9905 - val_loss: 0.0174 - val_acc: 0.9955\n",
      "Epoch 17/50\n",
      " 6000/24194 [======>.......................] - ETA: 31s - loss: 0.0183 - acc: 0.9955"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "patience = 4\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)\n",
    "    ]\n",
    "\n",
    "accuracy = 0\n",
    "#Fit the model\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "for user_groups in range(9):\n",
    "    x_train = x_train_list[user_groups]\n",
    "    x_test = x_test_list[user_groups]\n",
    "    y_train_hot = train_onehot_list[user_groups]\n",
    "    y_test_hot = test_onehot_list[user_groups]\n",
    "    history = model_m.fit(x_train,\n",
    "                  y_train_hot,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  callbacks=callbacks_list,\n",
    "                  validation_data=(x_test, y_test_hot),\n",
    "                  verbose=1)\n",
    "\n",
    "    best_val_acc =  history.history['val_acc'][-(patience+1)]\n",
    "    print(\"Best validation accuracy is:\",best_val_acc)\n",
    "    accuracy += best_val_acc\n",
    "    #When the fitting is done, reset the weights for the next round.\n",
    "    shuffle_weights(model_m)\n",
    "    # We want our users to \n",
    "\n",
    "accuracy = accuracy / 9        \n",
    "print(\"The average accuracy(expected value for our final model)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "patience = 4\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "accuracy = 0\n",
    "#Fit the model\n",
    "\n",
    "model_m = []\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "for user_groups in range(9):\n",
    "    model_m.append(Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs))\n",
    "\n",
    "for user_groups in range(9):\n",
    "    model_m[user_groups].compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = [i for i in range(9)]\n",
    "accuracy = []\n",
    "def single_headed(i, history, x_train, y_train_hot, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "         verbose=0, callbacks=callbacks_list, validation_data=(x_test, y_test_hot), output):\n",
    "    history = model_m[i].fit(x_train,\n",
    "                  y_train_hot,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  verbose=0,\n",
    "                  callbacks=callbacks_list,\n",
    "                  validation_data=(x_test, y_test_hot))\n",
    "    accuracy.append((i, history.history['val_acc'][-(patience+1)]))\n",
    "    \n",
    "output = mp.Queue()\n",
    "processes = [mp.Process(target=single_headed, args=(i, history[i], x_train_list[i], train_onehot_list[i],\n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "         verbose=0, callbacks=callbacks_list, validation_data=(x_test_list[i], test_onehot_list[i]), output)) for i in range(9)]\n",
    "\n",
    "for i in range(9):\n",
    "    processes[i].start()\n",
    "    print(\"Process started\")\n",
    "    print(\"Doing validation #\", i)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "acc = 0 \n",
    "for i in range(len(accuracy)):\n",
    "    acc += accuracy[i][1]\n",
    "    \n",
    "acc = acc / 9\n",
    "\n",
    "print(\"The expected accuracy from 9-fold validation is \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA\n",
    "Incorporate a multi-head model (with at least 3 heads) each using a different kernel size.  Do the averaging as above.   Can you come up with hyperparamters that beat the performance of the earlier 2-headed model (though that was measured with just a single fold).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input,Conv1D, MaxPooling1D,GlobalAveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.layers.merge import concatenate\n",
    "\n",
    "inputs1 = Input(shape=(80, 3))\n",
    "h1conv1 = Conv1D(filters = 100, kernel_size=10, activation='relu')(inputs1)\n",
    "h1conv2 = Conv1D(filters=100, kernel_size=10, activation='relu')(h1conv1)\n",
    "h1pool1 = MaxPooling1D(3)(h1conv2)\n",
    "h1glob1 = GlobalAveragePooling1D()(h1pool1)\n",
    "h1drop1 = Dropout(0.5)(h1glob1)\n",
    "\n",
    "inputs2 = Input(shape=(80,3))\n",
    "h2conv1 = Conv1D(filters=100, kernel_size=13, activation='relu')(inputs2)\n",
    "h2conv2 = Conv1D(filters=100, kernel_size=13, activation='relu')(h2conv1)\n",
    "h2pool1 = MaxPooling1D(3)(h2conv2)\n",
    "h2glob1 = GlobalAveragePooling1D()(h2pool1)\n",
    "h2drop1 = Dropout(0.5)(h2glob1)\n",
    "\n",
    "inputs3 = Input(shape=(80, 3))\n",
    "h3conv1 = Conv1D(filters=100, kernal_size=16, activation='relu')(inputs3)\n",
    "h3conv2 = Conv1D(filters=100, kernal_size=16, activation='relu')(inputs3)\n",
    "h3pool1 = MaxPooling1D(3)(h3conv2)\n",
    "h3glob1 = GlobalAveragePooling1D()(h3pool1)\n",
    "h3drop1 = Dropout(0.5)(h3glob1)\n",
    "\n",
    "merged = concatenate([h1drop1, h2drop1, h3drop1])\n",
    "\n",
    "dense1 = Dense(100, activation='relu')(merged)\n",
    "outputs = Dense(6, activation='softmax')(dense1)\n",
    "\n",
    "\n",
    "print(model_hydra.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "BATCH_SIZE = 400\n",
    "EPOCHS = 50\n",
    "patience = 4\n",
    "\n",
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience)]\n",
    "\n",
    "accuracy = 0\n",
    "#Fit the model\n",
    "\n",
    "model_hydra = []\n",
    "model_hydra.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "for user_groups in range(9):\n",
    "    model_hydra.append(Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs))\n",
    "\n",
    "for user_groups in range(9):\n",
    "    model_hydra[user_groups].compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = [i for i in range(9)]\n",
    "accuracy = []\n",
    "def multi_headed(i, history, x_train, y_train_hot, batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "         verbose=0, callbacks=callbacks_list, validation_data=(x_test, y_test_hot), output):\n",
    "    history = model_hydra[i].fit(x_train,\n",
    "                  y_train_hot,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=EPOCHS,\n",
    "                  verbose=0,\n",
    "                  callbacks=callbacks_list,\n",
    "                  validation_data=(x_test, y_test_hot))\n",
    "    accuracy.append((i, history.history['val_acc'][-(patience+1)]))\n",
    "    \n",
    "output = mp.Queue()\n",
    "processes = [mp.Process(target=multi_headed, args=(i, history[i], x_train_list[i], train_onehot_list[i],\n",
    "                batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
    "         verbose=0, callbacks=callbacks_list, validation_data=(x_test_list[i], test_onehot_list[i]), output)) for i in range(9)]\n",
    "\n",
    "for i in range(9):\n",
    "    processes[i].start()\n",
    "    print(\"Process started\")\n",
    "    print(\"Doing validation #\", i)\n",
    "\n",
    "for p in processes:\n",
    "    p.join()\n",
    "\n",
    "acc = 0 \n",
    "for i in range(len(accuracy)):\n",
    "    acc += accuracy[i][1]\n",
    "    \n",
    "acc = acc / 9\n",
    "\n",
    "print(\"The expected accuracy from 9-fold validation is \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
