{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adverserial Examples\n",
    "\n",
    "Consider an input $x$ with true label $y$, to a machine learning classifier model.  An **adversarial example** for this model would be an input $\\tilde{x}$ whose values are only slightly different from $x$ such that the model will misclassify it as a different label $\\tilde{y}$.\n",
    "\n",
    "The initial thought was that these examples had something to do with the nonlinearity inherent in complex machine learning models.   However, the paper [\"EXPLAINING AND HARNESSING\n",
    "ADVERSARIAL EXAMPLES\"](https://arxiv.org/pdf/1412.6572.pdf) by Ian J. Goodfellow, Jonathon Shlens and Christian Szegedy of Google, indicate that in fact this behaviour comes about due to the following:\n",
    "*  the **linear** behaviour of such networks\n",
    "*  the **high dimensionality** of the input spaces they operate in\n",
    "\n",
    "They also show that this leads to a relatively easy method for generating such examples.   In this workbook we will follow their prescription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "Here we will use the standard MNIST sample included with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train info (60000, 28, 28) (60000,)\n",
      "Test info (10000, 28, 28) (10000,)\n",
      "Test_images shape (10000, 784)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from keras.datasets import mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "short = False\n",
    "if short:\n",
    "    train_images = train_images[:7000,:]\n",
    "    train_labels = train_labels[:7000]\n",
    "    test_images = test_images[:3000,:]\n",
    "    test_labels = test_labels[:3000]\n",
    "#\n",
    "print(\"Train info\",train_images.shape, train_labels.shape)\n",
    "print(\"Test info\",test_images.shape, test_labels.shape)\n",
    "train_images = train_images.reshape((train_images.shape[0],784))\n",
    "train_images = train_images.astype('float32')/255\n",
    "\n",
    "test_images = test_images.reshape((test_images.shape[0],784))\n",
    "test_images = test_images.astype('float32')/255\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "train_labels_cat = to_categorical(train_labels)\n",
    "test_labels_cat = to_categorical(test_labels)\n",
    "\n",
    "print(\"Test_images shape\", test_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A fully connected network (FCN) MNIST Classifier\n",
    "The paper states that they can easily generate adverserial examples using a **shallow** MNIST classifier.   In this context, \"shallow\" typically means 1 hidden layer, and usually just refers to fully connected networks.   \"Deep learning\" uses multiple hidden layers and pooling techniques, and often refers to models which **learn** their features: convolutional neural networks are thus \"deep\".\n",
    "\n",
    "Given that, let's build a very simple single layer FCN.  Below we will also apply this to a \"deep\" model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 400)               314000    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                4010      \n",
      "=================================================================\n",
      "Total params: 318,010\n",
      "Trainable params: 318,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "summary None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 4s 60us/step - loss: 0.2781 - acc: 0.9218 - val_loss: 0.1421 - val_acc: 0.9586\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.1157 - acc: 0.9659 - val_loss: 0.0997 - val_acc: 0.9686\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0766 - acc: 0.9776 - val_loss: 0.0792 - val_acc: 0.9760\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0556 - acc: 0.9841 - val_loss: 0.0731 - val_acc: 0.9773\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 3s 57us/step - loss: 0.0420 - acc: 0.9874 - val_loss: 0.0641 - val_acc: 0.9786\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(400,activation='relu',input_shape=(28*28,)))\n",
    "network.add(layers.Dense(10,activation='softmax'))\n",
    "\n",
    "print(\"summary\",network.summary())\n",
    "\n",
    "network.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "#\n",
    "history = network.fit(train_images,train_labels_cat,epochs=5,batch_size=128,validation_data=(test_images,test_labels_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper display function\n",
    "We are going to want to display both are starting **true** example, as well as the **adverserial** example we will generate from that true example, so let's define a helper display function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_img(img):\n",
    "    img = img.reshape(28,28)\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick test of the network\n",
    "Lets pass in a single example from of test examples.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label:                  4\n",
      "Predicted label:             4\n",
      "Predicted probability:       0.98978066\n",
      "Predictions for all classes: [[6.3347463e-05 6.3997794e-07 3.3730743e-05 1.1642200e-07 9.8978066e-01\n",
      "  5.2809236e-07 7.1310322e-05 2.1597363e-04 9.8960254e-06 9.8238625e-03]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "#\n",
    "# Define the sample using the index into the numpy arrays\n",
    "sample = 4\n",
    "#\n",
    "# Get the individual prediction for a single sample in the test set\n",
    "test_image = test_images[sample].reshape(1,784)\n",
    "test_label = test_labels[sample]\n",
    "predictions = network.predict(test_image)\n",
    "#\n",
    "# Get the max probability\n",
    "prob = np.max(predictions, axis = 1)\n",
    "#\n",
    "# Get the predicted classes for each row\n",
    "predLabel = np.argmax(predictions, axis = 1)\n",
    "print(\"True label:                 \",test_label)\n",
    "print(\"Predicted label:            \",predLabel[0])\n",
    "print(\"Predicted probability:      \",prob[0])\n",
    "print(\"Predictions for all classes:\",predictions)\n",
    "#\n",
    "# Now display\n",
    "plot_img(test_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Adverserial images\n",
    "To generate adverserial images, we will start with a test image, and perturb it by a small amount $\\eta$.   How do we determine $\\eta$?   For our MNIST images, they are encoded using 8-bit precision, with a maximum of 255.   There we expect that changes to an image of the scale less than $\\eta$=1/255=0.004 should not be noticeable.  Formally, we will set:\n",
    "$$\\tilde{x} = x + \\eta$$\n",
    "Looking at how this might interact with the weights $w$ in a network, we can see that:\n",
    "$$w^T\\tilde{x} = w^T x + w^T\\eta$$\n",
    "If the average value of the weights is $m$, and the dimension of the weights is $n$, then the impact of such a perturbation is $mn\\eta$.  This implies that we can make many small changes to the input $x$, which leave it relatively unchanged, while imparting a large change to the output of the network.   The authors of the above paper call this \"accidental steganography\": steganography is the practice of concealing a file, message, image, or video within another file, message, image, or video.\n",
    "\n",
    "The authors suggest this method to determine $\\eta$.  Let θ be the parameters of the model, x the input to the model, y the targets associated with x, and J(θ, x, y) be the cost used to train the neural network, then:\n",
    "$$\\eta = \\epsilon ~sign (\\triangledown_x J(θ, x, y))$$\n",
    "where the \"sign\" function simply returns the sign of the argument and $\\epsilon$ is a free parameter which controls the size of the step.   The authors suggest numbers like 0.1-0.25, but we will use smaller values like 0.02.\n",
    "\n",
    "To calculate the gradient of the loss with respect to the input, we will follow exactly the same idea as used in the \"Deep Learning\" book: section 5.4.2 \"Visualizing covnet filters\".\n",
    "\n",
    "Read through the code and comments below to make sure you follow the procedure, at least conceptually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Predictions [[3.8040540e-08 9.9903381e-01 1.7995112e-06 1.4389487e-06 8.6556181e-05\n",
      "  2.1652336e-07 1.9202658e-07 8.6158112e-04 1.3456788e-05 8.7864333e-07]]\n",
      "Test label 4\n",
      "Pred class [1]\n",
      "0 [1] [[6.0405392e-07 8.5803533e-01 5.6488843e-06 2.7780634e-05 6.6160853e-04\n",
      "  3.1404215e-06 8.0052223e-07 1.4100136e-01 2.3582506e-04 2.7837177e-05]]\n",
      "1 [7] [[3.31651279e-07 3.01940441e-02 1.01954220e-06 3.03618799e-05\n",
      "  1.05378160e-04 1.85673582e-06 1.17291016e-07 9.69514310e-01\n",
      "  1.29310036e-04 2.34036706e-05]]\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "# Initialize adversarial example with input image\n",
    "sample = 5\n",
    "test_image = test_images[sample].reshape((1,784))\n",
    "x = test_image\n",
    "x_adv = x\n",
    "print(type(x_adv))\n",
    "predictions = network.predict(x_adv)\n",
    "#\n",
    "# Get the max probabilites for each rows\n",
    "probs = np.max(predictions, axis = 1)\n",
    "#\n",
    "# Get the predicted classes for each row\n",
    "predclass = np.argmax(predictions, axis = 1)\n",
    "print(\"Predictions\",predictions)\n",
    "print(\"Test label\",test_label)\n",
    "print(\"Pred class\",predclass)\n",
    "#\n",
    "# Set variables\n",
    "epochs = 15\n",
    "epsilon = 0.02\n",
    "#\n",
    "# The target is a one-hot array of 1x10\n",
    "target = np.zeros((1,10))\n",
    "#\n",
    "# Set the target to its true value for the real input\n",
    "target[0,predclass[0]] = 1\n",
    "#\n",
    "# Define the loss function\n",
    "loss = K.categorical_crossentropy(target, network.output)\n",
    "#\n",
    "# Get the gradient of the cost function with respect to the *input*\n",
    "grads = K.gradients(loss, network.input)[0]\n",
    "#\n",
    "# This next line **defines** the \"iterate\" function using the \"K.function\" method.\n",
    "# This takes the given network input and calculates the loss and the gradient.\n",
    "iterate = K.function([network.input],[loss,grads])\n",
    "#\n",
    "# Now actually do the calculations\n",
    "for i in range(epochs): \n",
    "#\n",
    "# Get the loss and gradient value given our input image\n",
    "    loss_value, grads_value = iterate([x_adv])\n",
    "#\n",
    "# Update the input - make or small changes here!\n",
    "    x_adv = x_adv + epsilon*K.sign(grads_value)\n",
    "#\n",
    "# Now we can get the prediction given our modified image\n",
    "    preds = network.predict(x_adv,steps=1)\n",
    "    print(i, np.argmax(preds, axis = 1),preds)\n",
    "#\n",
    "# Once our predicted class is no longer the same as our starting class, jump out of the loop\n",
    "    if predclass != np.argmax(preds, axis = 1)[0]:\n",
    "        break\n",
    "#\n",
    "# Convert from a tensorflow tensor to a numpy array\n",
    "print(type(x_adv))\n",
    "x_adv = K.eval(x_adv)\n",
    "#print(x_adv)\n",
    "#\n",
    "# Plot the starting image, the modified image, and their difference\n",
    "plot_img(x)   \n",
    "plot_img(x_adv)   \n",
    "plot_img(x_adv-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Tasks\n",
    "Here are some things to try:\n",
    "1.  Try different example inputs - see if the adverserial examples are more difficult to generate for certain digits.\n",
    "2.  Try playing with different $\\epsilon$ values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Less Simple Task\n",
    "Modify the code blocks below to make a simple CNN classifier for MNIST.   Train it and then use the same procedure as above to generate adverserial examples (that code is repeated as well below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d407796617dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels_cat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels_cat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1252\u001b[0;31m                 \u001b[0;34m'This model has not yet been built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1253\u001b[0m                 \u001b[0;34m'Build the model first by calling build() '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m                 \u001b[0;34m'or calling fit() with some data. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. "
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "#\n",
    "# Your code here!\n",
    "\n",
    "network.add(layers.Flatten())\n",
    "network.add(layers.Dense(10,activation='softmax'))\n",
    "network.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "print(network.summary())\n",
    "#\n",
    "history = network.fit(train_images,train_labels_cat,epochs=5,batch_size=128,validation_data=(test_images,test_labels_cat))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate adverserial examples for CNN\n",
    "After fitting your CNN above, you will have to make a trivial change to the code below to get it to work with that network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "# Initialize adversarial example with input image\n",
    "sample = 5\n",
    "test_image = test_images[sample].reshape((1,784))\n",
    "x = test_image\n",
    "x_adv = x\n",
    "predictions = network.predict(x_adv)\n",
    "#\n",
    "# Get the max probabilites for each rows\n",
    "probs = np.max(predictions, axis = 1)\n",
    "#\n",
    "# Get the predicted classes for each row\n",
    "predclass = np.argmax(predictions, axis = 1)\n",
    "print(\"Predictions\",predictions)\n",
    "print(\"Test label\",test_label)\n",
    "print(\"Pred class\",predclass)\n",
    "#\n",
    "# Set variables\n",
    "epochs = 15\n",
    "epsilon = 0.02\n",
    "#\n",
    "# The target is a one-hot array of 1x10\n",
    "target = np.zeros((1,10))\n",
    "#\n",
    "# Set the target to its true value for the real input\n",
    "target[0,predclass[0]] = 1\n",
    "#\n",
    "# Define the loss function\n",
    "loss = K.categorical_crossentropy(target, network.output)\n",
    "#\n",
    "# Get the gradient of the cost function with respect to the *input*\n",
    "grads = K.gradients(loss, network.input)[0]\n",
    "#\n",
    "# This next line **defines** the \"iterate\" function using the \"K.function\" method.\n",
    "# This takes the given network input and calculates the loss and the gradient.\n",
    "iterate = K.function([network.input],[loss,grads])\n",
    "#\n",
    "# Now actually do the calculations\n",
    "for i in range(epochs): \n",
    "#\n",
    "# Get the loss and gradient value given our input image\n",
    "    loss_value, grads_value = iterate([x_adv])\n",
    "#\n",
    "# Update the input - make or small changes here!\n",
    "    x_adv = x_adv + epsilon*K.sign(grads_value)\n",
    "#\n",
    "# Now we can get the prediction given our modified image\n",
    "    preds = network.predict(x_adv,steps=1)\n",
    "    print(i, np.argmax(preds, axis = 1),preds)\n",
    "#\n",
    "# Once our predicted class is no longer the same as our starting class, jump out of the loop\n",
    "    if predclass != np.argmax(preds, axis = 1)[0]:\n",
    "        break\n",
    "#\n",
    "# Convert from a tensorflow tensor to a numpy array\n",
    "x_adv = K.eval(x_adv)\n",
    "#print(x_adv)\n",
    "#\n",
    "# Plot the starting image, the modified image, and their difference\n",
    "plot_img(x)   \n",
    "plot_img(x_adv)   \n",
    "plot_img(x_adv-x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (Conda 5.2) [python/3.6-conda5.2]",
   "language": "python",
   "name": "sys_python36conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
